all:
'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],
'activation': ['identity', 'logistic', 'tanh', 'relu'],
'tol': [1e-5, 1e-4, 1e-3, 1e-2],
'hidden_layer_sizes': z

adam: ('max_iter': 1000)
'learning_rate_init': [1e-4, 1e-3, 1e-2, 1e-1],
'shuffle': [true, false],
'beta_1': [0.7, 0.8, 0.9, 0.95],
'beta_2': [0.8, 0.9, 0.95, 0.99, 0.999]

l-bfgs:
'max_fun': [10000, 15000, 20000]